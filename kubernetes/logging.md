# Kubernetes logging

We use the following components for our logging stack:

- [Fluentd](https://www.fluentd.org/)
- [AWS Cloudwatch logs](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html)
- [AWS Elasticsearch](https://aws.amazon.com/elasticsearch-service/)
- [Kibana](https://www.elastic.co/products/kibana)

## Architecture

Logs are picked up by a Fluentd `DaemonSet` that is running on every Kubernetes node. We have a default config to pick up all container logs and ship those to CloudWatch logs. We use CloudWatch logs as a buffer and archiving system. On CloudWatch logs we have a subscription stream active that streams the logs to AWS Elasticsearch with an Lambda function. Once they are in AWS Elasticsearch, we can access the logs with Kibana. You'll find your Kibana address in the `README.md` file of your GitHub repo, it'll be something like `https://kibana.staging.yourcompanyname.com` and `https://kibana.production.yourcompanyname.com`.

![Architecture](images/logging_k8s.png "Architecture")

## Log sources

As of now, the logs that are picked up by Fluentd are the same that are available through `kubectl logs`, which are the `stdout` and `stderr` streams of all containers running in a Pod. In other words, everything that's dumped in `stdout` and `stderr` of every container will be available in Kibana.

Under the hood, Kubernetes uses the default Docker log driver, which is the [`json-file`](https://docs.docker.com/config/containers/logging/json-file/#usage). This is the first component that your logs will go through, so you should be aware of how this driver treats and modifies your logs.

Additionally, there are some other system logs that are also picked up and available in Kibana, such as the `kubelet` and `dockerd` service logs, the startup script, etc.

## Log formatting

All log lines are turned into a JSON map with some added metadata:

- `log`: the original plain text line
- `stream`: either `stdout` or `stderr`
- `time`: the time when the log entry was ingested

**Note**: The Docker json logging driver treats each line as a separate message. When using the Docker logging driver, there is no direct support for multi-line messages.

### Time

The time of your final log entries will be the one added by the Docker logging driver at the time of ingestion, which can suppose a small difference with the time when the log entry was generated, normally no longer than a few microseconds or a couple of milliseconds.

### JSON log entries

If your original log entries are JSON maps, those entries are escaped and set as-is in the `log` attribute of the JSON map generated by the Docker logging driver.

In a later stage, when the logs end up in Elasticsearch, the `log` attribute that contains an escaped JSON map will be expanded and will be available as fields of the log entry. Here's an example:

The container logs the following to `stdout`:

```json
{"foo":"bar","level":"info"}
```

The Docker logging plugin will log that as:

```json
{"log":"{\"foo\":\"bar\",\"level\":\"info\"}","stream":"stdout","time":"2019-01-03T12:09:18.844939857Z"}
```

Afterwards, Fluentd adds some Kubernetes metadata and it ships it to CloudWatch logs like:

```json
{
  "log": "{\"foo\":\"bar\",\"level\":\"info\"}",
  "stream": "stdout",
  "docker": {
    "container_id": "7ff0c30169ef833873ee1c67db483f71638e55fb6e7cd3334b9ef749a11df744"
  },
  "kubernetes": {
    "container_name": "something",
    "namespace_name": "default",
    "pod_name": "something",
    "container_image": "sha256:750279afe98f6e71f4adb29b1c025a21f1428c1d25211bbcf1e30f2d81ead4d2",
    "container_image_id": "docker-pullable://quay.io/skyscrapers/something@sha256:e2fd60ff0ae4500a75b80ebaa30e0e7deba9ad107833e8ca53f0047c42c5a057",
    "pod_id": "7ff0c30169ef833873ee1c67db483f71638e55fb6e7cd3334b9ef749a11df744",
    "labels": {
      "app": "example-app",
      "component": "app",
      "release": "example-app"
    },
    "host": "ip-10-19-208-214.eu-west-1.compute.internal",
    "master_url": "https://100.64.0.1:443/api",
    "namespace_id": "bb014d55-0f66-11e9-bfbe-0a01d59392ae",
    "namespace_labels": {
      "prometheus": "true"
    }
  }
}
```

And finally, it ends up in Kibana / Elasticsearch like:

```json
{
  "_index": "cwl-2019.01.04",
  "_type": "kubernetes",
  "_id": "34490501621967913818452806894023292948478156754441666560",
  "_version": 1,
  "_score": null,
  "_source": {
    "log": "{\"foo\":\"bar\",\"level\":\"info\"}",
    "stream": "stdout",
    "docker": {
      "container_id": "7ff0c30169ef833873ee1c67db483f71638e55fb6e7cd3334b9ef749a11df744"
    },
    "kubernetes": {
      "container_name": "something",
      "namespace_name": "default",
      "pod_name": "something",
      "container_image": "sha256:750279afe98f6e71f4adb29b1c025a21f1428c1d25211bbcf1e30f2d81ead4d2",
      "container_image_id": "docker-pullable://quay.io/skyscrapers/something@sha256:e2fd60ff0ae4500a75b80ebaa30e0e7deba9ad107833e8ca53f0047c42c5a057",
      "pod_id": "7ff0c30169ef833873ee1c67db483f71638e55fb6e7cd3334b9ef749a11df744",
      "labels": {
        "app": "example-app",
        "component": "app",
        "release": "example-app"
      },
      "host": "ip-10-19-208-214.eu-west-1.compute.internal",
      "master_url": "https://100.64.0.1:443/api",
      "namespace_id": "bb014d55-0f66-11e9-bfbe-0a01d59392ae",
      "namespace_labels": {
        "prometheus": "true"
      }
    },
    "log_json": {
      "foo": "bar",
      "level": "info"
    },
    "@id": "34490501621967913818452806894023292948478156754441666560",
    "@timestamp": "2019-01-03T12:09:18.844Z",
    "@message": "{\"log\":\"{\\\"foo\\\":\\\"bar\\\",\\\"level\\\":\\\"info\\\"}\",\"stream\":\"stdout\",\"docker\":{\"container_id\":\"7ff0c30169ef833873ee1c67db483f71638e55fb6e7cd3334b9ef749a11df744\"},\"kubernetes\":{\"container_name\":\"something\",\"namespace_name\":\"default\",\"pod_name\":\"something\",\"container_image\":\"sha256:750279afe98f6e71f4adb29b1c025a21f1428c1d25211bbcf1e30f2d81ead4d2\",\"container_image_id\":\"docker-pullable://quay.io/skyscrapers/something@sha256:e2fd60ff0ae4500a75b80ebaa30e0e7deba9ad107833e8ca53f0047c42c5a057\",\"pod_id\":\"7ff0c30169ef833873ee1c67db483f71638e55fb6e7cd3334b9ef749a11df744\",\"labels\":{\"app\":\"example-app\",\"component\":\"app\",\"release\":\"example-app\"},\"host\":\"ip-10-19-208-214.eu-west-1.compute.internal\",\"master_url\":\"https://100.64.0.1:443/api\",\"namespace_id\":\"bb014d55-0f66-11e9-bfbe-0a01d59392ae\",\"namespace_labels\":{\"prometheus\":\"true\"}}}",
    "@owner": "12345678901",
    "@log_group": "kubernetes",
    "@log_stream": "kubernetes.var.log.containers.something-848c6dfb58-t2cbg_default_something-1b0f48647e291a554f1fac594be8e07acbe817464a3d4b35b390d3b9766fd146.log"
  },
  "fields": {
    "@timestamp": [
      "2019-01-03T12:09:18.844Z"
    ]
  },
  "sort": [
    1546607582613
  ]
}
```

As you can see, the original JSON map is in the `_source.log_json` attribute of the final Kibana entry.

## Log browsing and filtering

To learn how to use Kibana and all its features, the best place to start is the official [Kibana user guide](https://www.elastic.co/guide/en/kibana/6.5/discover.html). In there you'll find the basics on how to browse and filter your logs, different ways to visualize them and how to create useful dashboards.

Once you are familiar with the basics, you'll be able to use all the added Kubernetes metadata to get the most out of Kibana. You can filter by all the fields that are indexed. Here are some examples:

You can add a filter to show only the logs of a given pod:

```yaml
kubernetes.pod_name: "your-pod-name"
```

Or by namespace:

```yaml
kubernetes.namespace: "your-application"
```

By a Helm release:

```yaml
kubernetes.labels.release: "example-app"
```

You can even add filters on attributes from your JSON logs, if those attributes are indexed:

```yaml
log_json.statusCode: 200
```
